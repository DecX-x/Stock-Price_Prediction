{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import  mean_absolute_percentage_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras import layers\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL:\n",
    "    \"\"\"\n",
    "    ticker: str\n",
    "    period: string\n",
    "    test_size: float betwee 0 and 1\n",
    "    n_input: int\n",
    "    timestep: int\n",
    "    Extracts data for stock with ticker `ticker` from yf api,\n",
    "    splits the data into train and test sets by date,\n",
    "    reshapes the data into np.array of shape [#weeks, 5, 1],\n",
    "    converts our problem into supervised learning problem.\n",
    "    \"\"\"\n",
    "    def  __init__(self, ticker, test_size=0.2, period='max', n_input=5, timestep=5) -> None:\n",
    "        self.ticker = ticker\n",
    "        self.period = period\n",
    "        self.test_size = test_size\n",
    "        self.n_input = n_input\n",
    "        self.df = self.extract_historic_data()\n",
    "        self.timestep = timestep\n",
    "        self.train, self.test = self.etl()\n",
    "        self.X_train, self.y_train = self.to_supervised(self.train)\n",
    "        self.X_test, self.y_test = self.to_supervised(self.test)\n",
    "\n",
    "    def extract_historic_data(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        gets historical data from yf api.\n",
    "        \"\"\"\n",
    "        t = yf.Ticker(self.ticker)\n",
    "        history = t.history(period=self.period)\n",
    "        return history.Close\n",
    "    def split_data(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Splits our pd.Series into train and test series with\n",
    "        test series representing test_size * 100 % of data.\n",
    "        \"\"\"\n",
    "        data = self.extract_historic_data()\n",
    "        if len(data) != 0:\n",
    "            train_idx = round(len(data) * (1-self.test_size))\n",
    "            train = data[:train_idx]\n",
    "            test = data[train_idx:]\n",
    "            train = np.array(train)\n",
    "            test = np.array(test)\n",
    "            return train[:, np.newaxis], test[:, np.newaxis]\n",
    "        else:\n",
    "            raise Exception('Data set is empty, cannot split.')\n",
    "\n",
    "    def window_and_reshape(self, data) -> np.array:\n",
    "        \"\"\"\n",
    "        Reformats data into shape our model needs,\n",
    "        namely, [# samples, timestep, # feautures]\n",
    "        samples\n",
    "        \"\"\"\n",
    "        NUM_FEATURES = 1\n",
    "        samples = int(data.shape[0] / self.timestep)\n",
    "        result = np.array(np.array_split(data, samples))\n",
    "        return result.reshape((samples, self.timestep, NUM_FEATURES))\n",
    "\n",
    "    def transform(self, train, test) -> np.array:\n",
    "        train_remainder = train.shape[0] % self.timestep\n",
    "        test_remainder = test.shape[0] % self.timestep\n",
    "        if train_remainder != 0 and test_remainder != 0:\n",
    "            train = train[train_remainder:]\n",
    "            test = test[test_remainder:]\n",
    "        elif train_remainder != 0:\n",
    "            train = train[train_remainder:]\n",
    "        elif test_remainder != 0:\n",
    "            test = test[test_remainder:]\n",
    "        return self.window_and_reshape(train), self.window_and_reshape(test)\n",
    "    def etl(self) -> tuple[np.array, np.array]:\n",
    "        \"\"\"\n",
    "        Runs complete ETL\n",
    "        \"\"\"\n",
    "        train, test = self.split_data()\n",
    "        return self.transform(train, test)\n",
    "\n",
    "    def to_supervised(self, train, n_out=5) -> tuple:\n",
    "        \"\"\"\n",
    "        Converts our time series prediction problem to a\n",
    "        supervised learning problem.\n",
    "        \"\"\"\n",
    "        # flatted the data\n",
    "        data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "        X, y = [], []\n",
    "        in_start = 0\n",
    "        # step over the entire history one time step at a time\n",
    "        for _ in range(len(data)):\n",
    "            # define the end of the input sequence\n",
    "            in_end = in_start + self.n_input\n",
    "            out_end = in_end + n_out\n",
    "            # ensure we have enough data for this instance\n",
    "            if out_end <= len(data):\n",
    "                x_input = data[in_start:in_end, 0]\n",
    "                x_input = x_input.reshape((len(x_input), 1))\n",
    "                X.append(x_input)\n",
    "                y.append(data[in_end:out_end, 0])\n",
    "                # move along one time step\n",
    "                in_start += 1\n",
    "        return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictAndForecast:\n",
    "    \"\"\"\n",
    "    model: tf.keras.Model\n",
    "    train: np.array\n",
    "    test: np.array\n",
    "    Takes a trained model, train, and test datasets and returns predictions\n",
    "    of len(test) with same shape.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train, test, n_input=5) -> None:\n",
    "        self.model = model\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.n_input = n_input\n",
    "        self.predictions = self.get_predictions()\n",
    "\n",
    "    def forecast(self, history) -> np.array:\n",
    "        \"\"\"\n",
    "        Given last weeks actual data, forecasts next weeks prices.\n",
    "        \"\"\"\n",
    "        # flatten data\n",
    "        data = np.array(history)\n",
    "        data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "        # retrieve last observations for input data\n",
    "        input_x = data[-self.n_input:, :]\n",
    "        # reshape into [1, n_input, 1]\n",
    "        input_x = input_x.reshape((1, len(input_x), input_x.shape[1]))\n",
    "        # forecast the next week\n",
    "        yhat = self.model.predict(input_x, verbose=0)\n",
    "        # we only want the vector forecast\n",
    "        yhat = yhat[0]\n",
    "        return yhat\n",
    "    def get_predictions(self) -> np.array:\n",
    "        \"\"\"\n",
    "        compiles models predictions week by week over entire\n",
    "        test set.\n",
    "        \"\"\"\n",
    "        # history is a list of weekly data\n",
    "        history = [x for x in self.train]\n",
    "        # walk-forward validation over each week\n",
    "        predictions = []\n",
    "        for i in range(len(self.test)):\n",
    "            yhat_sequence = self.forecast(history)\n",
    "            # store the predictions\n",
    "            predictions.append(yhat_sequence)\n",
    "        # get real observation and add to history for predicting the next week\n",
    "            history.append(self.test[i, :])\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate:\n",
    "\n",
    "  def __init__(self, actual, predictions) -> None:\n",
    "    self.actual = actual\n",
    "    self.predictions = predictions\n",
    "    self.var_ratio = self.compare_var()\n",
    "    self.mape = self.evaluate_model_with_mape()\n",
    "\n",
    "  def compare_var(self):\n",
    "    return abs( 1 - (np.var(self.predictions) / np.var(self.actual)))\n",
    "\n",
    "  def evaluate_model_with_mape(self):\n",
    "    return mean_absolute_percentage_error(self.actual.flatten(), self.predictions.flatten())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ETL('AAPL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(etl: ETL, epochs=25, batch_size=32) -> tf.keras.Model:\n",
    "  \"\"\"\n",
    "  Builds, compiles, and fits our LSTM baseline model.\n",
    "  \"\"\"\n",
    "  n_timesteps, n_features, n_outputs = 5, 1, 5\n",
    "  callbacks = [tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "  model.add(Dense(50, activation='relu'))\n",
    "  model.add(Dense(n_outputs))\n",
    "  print('compiling baseline model...')\n",
    "  model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mape'])\n",
    "  print('fitting model...')\n",
    "  start = time.time()\n",
    "  history = model.fit(etl.X_train, etl.y_train, batch_size=batch_size, epochs=epochs, validation_data=(etl.X_test, etl.y_test), verbose=1, callbacks=callbacks)\n",
    "  print(time.time() - start)\n",
    "  return model, history\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
